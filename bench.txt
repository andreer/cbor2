# CBOR2 Benchmarking Guide

## Setup

1. Install benchmark dependencies:
   ```bash
   uv pip install pytest-benchmark orjson
   ```

2. Fix pytest-cov compatibility issue (if not already fixed):
   The benchmarks/tests/conftest.py file needs to check if pytest-cov is loaded:
   ```python
   def pytest_configure(config):
       cov = config.pluginmanager.get_plugin("_cov")
       if cov:  # Add this check
           cov.options.no_cov = True
           if cov.cov_controller:
               cov.cov_controller.pause()
   ```

## Running Benchmarks

### Quick benchmarks (compare cbor2 vs json):
```bash
cd benchmarks
uv run pytest --benchmark-only tests/test_vs_json.py
```

### Save baseline for comparison:
```bash
cd benchmarks
uv run pytest --benchmark-only --benchmark-json=baseline.json tests/test_vs_json.py
```

### Compare with baseline:
```bash
# After making changes and rebuilding:
uv pip install -e . --force-reinstall
cd benchmarks
uv run pytest --benchmark-only --benchmark-json=current.json tests/test_vs_json.py
```

### Filter specific tests:
```bash
# Only deserialization tests:
uv run pytest --benchmark-only tests/test_vs_json.py::test_loads

# Only cbor2 (not json):
uv run pytest --benchmark-only tests/test_vs_json.py -k "ccbor2"

# Specific dataset:
uv run pytest --benchmark-only tests/test_vs_json.py -k "256 ASCII"
```

## Comparing Git Versions

### Compare current HEAD with a specific commit:
```bash
# Save current version
git stash  # if you have uncommitted changes
uv pip install -e . --force-reinstall
cd benchmarks
uv run pytest --benchmark-only --benchmark-json=current.json tests/test_vs_json.py

# Test old version
git checkout <commit-hash>
uv pip install -e . --force-reinstall
cd benchmarks
uv run pytest --benchmark-only --benchmark-json=baseline.json tests/test_vs_json.py

# Return to current
git checkout master
git stash pop  # if you stashed
```

### Analyze JSON results with Python:
```python
import json

with open('baseline.json') as f:
    baseline = json.load(f)
with open('current.json') as f:
    current = json.load(f)

for bench_b in baseline['benchmarks']:
    if 'ccbor2' in bench_b['name']:
        for bench_c in current['benchmarks']:
            if bench_c['name'] == bench_b['name']:
                name = bench_b['param'].replace('ccbor2-', '')
                baseline_mean = bench_b['stats']['mean'] * 1_000_000  # to μs
                current_mean = bench_c['stats']['mean'] * 1_000_000
                speedup = (baseline_mean / current_mean - 1) * 100
                print(f"{name:30} {baseline_mean:8.2f}μs -> {current_mean:8.2f}μs ({speedup:+6.1f}%)")
```

## Large File Benchmark (from Discussion #261)

Test with the 5MB JSON file used in the performance discussion:

```bash
# Download test file
curl -o /tmp/5MB-min.json https://microsoftedge.github.io/Demos/json-dummy-data/5MB-min.json

# Create benchmark script
cat > /tmp/bench_large.py << 'EOF'
from timeit import timeit
import json
import orjson
import cbor2
from pathlib import Path

data = orjson.loads(Path('/tmp/5MB-min.json').read_bytes())
Path('/tmp/5MB-min.cbor').write_bytes(cbor2.dumps(data))

def load_json():
    path = Path('/tmp/5MB-min.json')
    json.loads(path.read_bytes())

def load_orjson():
    path = Path('/tmp/5MB-min.json')
    orjson.loads(path.read_bytes())

def load_cbor():
    path = Path('/tmp/5MB-min.cbor')
    cbor2.loads(path.read_bytes())

def load_cbor_open():
    with open('/tmp/5MB-min.cbor', 'rb') as fp:
        cbor2.load(fp)

print('json', timeit(load_json, number=100))
print('orjson', timeit(load_orjson, number=100))
print('cbor', timeit(load_cbor, number=100))
print('cbor_open', timeit(load_cbor_open, number=100))
EOF

# Run benchmark
.venv/bin/python /tmp/bench_large.py
```

## Important Notes

1. **Always rebuild** after checking out a different commit:
   ```bash
   uv pip install -e . --force-reinstall
   ```

2. **Warmup**: pytest-benchmark handles warmup automatically

3. **Result variation**: Run multiple times and look at the min/median, not just mean

4. **System load**: Close other applications for consistent results

5. **The test data**:
   - "composite object": Mixed types (strings, numbers, dicts, lists)
   - "256 ASCII array": Short ASCII strings
   - "256 unicode array": UTF-8 strings (Arabic)
   - "256 doubles array": Floating point numbers
   - "complex object": Nested user/friend structures

## Interpreting Results

### Good improvements:
- String-heavy workloads: 15-25% faster expected from string optimization
- Large files: ~40% faster from buffering improvements

### Expected no change:
- Number arrays (doubles, ints): No string decoding involved
- Boolean arrays: Trivial to decode

### Red flags:
- Any regression > 5%: Investigate!
- Inconsistent results: System load or measurement noise
